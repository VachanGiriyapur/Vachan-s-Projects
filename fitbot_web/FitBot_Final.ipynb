{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oaqggIJ90LcM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader as DL\n",
        "import torch.utils.data\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PifA28hD0YIF"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"/content/big_fitness_dataset.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pQrQKdT70YKn"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(glove_path=\"/content/glove.6B.100d.txt\"):\n",
        "    glove_dict = {}\n",
        "    with open(glove_path, 'r', encoding=\"utf8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = torch.tensor([float(val) for val in parts[1:]], dtype=torch.float32)\n",
        "            glove_dict[word] = vector\n",
        "    return glove_dict\n",
        "\n",
        "glove_dict = load_glove_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fV7YTHC0YNM",
        "outputId": "2fd07889-0ec0-4198-e4bf-3ff4f4b43036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded and cleaned 10110 questions and 10110 answers.\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/big_fitness_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "questions, answers = [], []\n",
        "for i in range(0, len(lines), 2):\n",
        "    if i + 1 < len(lines) and lines[i].startswith(\"Q:\") and lines[i + 1].startswith(\"A:\"):\n",
        "        # Clean questions\n",
        "        q = lines[i].replace(\"Q:\", \"\").strip().lower()\n",
        "        # Remove multiple spaces\n",
        "        q = \" \".join(q.split())\n",
        "        # Ensure question ends with question mark if it's an actual question\n",
        "        if any(q.startswith(w) for w in [\"what\", \"how\", \"why\", \"when\", \"where\", \"which\", \"can\", \"should\", \"is\", \"are\", \"do\", \"does\"]) and not q.endswith(\"?\"):\n",
        "            q += \"?\"\n",
        "\n",
        "        # Clean answers\n",
        "        a = lines[i + 1].replace(\"A:\", \"\").strip().lower()\n",
        "        # Remove multiple spaces\n",
        "        a = \" \".join(a.split())\n",
        "        # Ensure answers are complete sentences with proper punctuation\n",
        "        if not a.endswith(('.', '!', '?')):\n",
        "            a += '.'\n",
        "        # Capitalize first letter of answer\n",
        "        if a:\n",
        "            a = a[0].upper() + a[1:]\n",
        "\n",
        "        # Skip very short or low-quality pairs\n",
        "        if len(q.split()) < 2 or len(a.split()) < 3:\n",
        "            continue\n",
        "\n",
        "        questions.append(q)\n",
        "        answers.append(a)\n",
        "\n",
        "print(f\"Loaded and cleaned {len(questions)} questions and {len(answers)} answers.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WPY0M5_20YP8"
      },
      "outputs": [],
      "source": [
        "special_tokens = [\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]\n",
        "word_freq = Counter(\" \".join(questions + answers).split())\n",
        "vocab = special_tokens + sorted(word_freq.keys())\n",
        "word2index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index2word = {idx: word for word, idx in word2index.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDsqP_JJ0YSm",
        "outputId": "8ddcf836-0fa2-4f64-abd1-05b6496d378c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Encoded Q: [1, 14836, 2085, 13754, 2543, 9162, 11118, 5411, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded Q: ['<sos>', 'what', 'are', 'the', 'benefits', 'of', 'regular', 'exercise?', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "def encode_text(text, word_map, max_len=20):\n",
        "    tokens = text.strip().lower().split()\n",
        "    encoded = [word_map.get(word, word_map[\"<unk>\"]) for word in tokens]\n",
        "    encoded = [word_map[\"<sos>\"]] + encoded[:max_len - 2] + [word_map[\"<eos>\"]]\n",
        "    return encoded + [word_map[\"<pad>\"]] * (max_len - len(encoded))\n",
        "\n",
        "encoded_questions = [encode_text(q, word2index) for q in questions]\n",
        "encoded_answers = [encode_text(a, word2index) for a in answers]\n",
        "\n",
        "print(\"Sample Encoded Q:\", encoded_questions[0])\n",
        "print(\"Decoded Q:\", [index2word[i] for i in encoded_questions[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XaDjr_mO0YVa"
      },
      "outputs": [],
      "source": [
        "def create_embedding_matrix(word2index, glove_dict, embedding_dim=100):\n",
        "    matrix = torch.randn(len(word2index), embedding_dim) * 0.1\n",
        "    for word, idx in word2index.items():\n",
        "        if word in glove_dict:\n",
        "            matrix[idx] = glove_dict[word]\n",
        "    return matrix\n",
        "\n",
        "embedding_matrix = create_embedding_matrix(word2index, glove_dict, embedding_dim=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9Jcq_yLY0Yal"
      },
      "outputs": [],
      "source": [
        "class FitnessDataset(Dataset):\n",
        "    def __init__(self, questions, answers):\n",
        "        self.questions = questions\n",
        "        self.answers = answers\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.questions[idx]), torch.tensor(self.answers[idx])\n",
        "\n",
        "dataset = FitnessDataset(encoded_questions, encoded_answers)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DL(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DL(val_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8U31aIf40YdN"
      },
      "outputs": [],
      "source": [
        "class FitnessGRUBot(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=256, num_layers=2, dropout=0.3, pretrained_embeddings=None):\n",
        "        super(FitnessGRUBot, self).__init__()\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "            self.embedding.weight.requires_grad = True  # Allow fine-tuning\n",
        "\n",
        "        # Encoder GRU\n",
        "        self.encoder = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            bidirectional=True  # Bidirectional for better context understanding\n",
        "        )\n",
        "\n",
        "        # Decoder GRU - Correct indentation here\n",
        "        self.decoder = nn.GRU(\n",
        "            input_size=embed_dim,\n",
        "            hidden_size=hidden_dim * 2,  # Account for bidirectional encoder\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=dropout if num_layers > 1 else 0\n",
        "        )\n",
        "\n",
        "        # Attention mechanism - simple dot product attention\n",
        "        self.attention = nn.Linear(hidden_dim * 2, hidden_dim * 2) # Correct indentation here as well\n",
        "\n",
        "        # Output projection\n",
        "        self.fc_out = nn.Linear(hidden_dim * 2, vocab_size)\n",
        "\n",
        "        # Additional dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Initialize weights\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name and 'embedding' not in name:\n",
        "                nn.init.xavier_uniform_(param)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # Source and target shape: [batch_size, seq_len]\n",
        "        batch_size = src.shape[0]\n",
        "        # Embed source and target\n",
        "        src_embedded = self.dropout(self.embedding(src))  # [batch_size, src_len, embed_dim]\n",
        "        tgt_embedded = self.dropout(self.embedding(tgt))  # [batch_size, tgt_len, embed_dim]\n",
        "\n",
        "        # Encode source\n",
        "        encoder_outputs, encoder_hidden = self.encoder(src_embedded)\n",
        "        # encoder_outputs: [batch_size, src_len, hidden_dim*2]\n",
        "        # encoder_hidden: [num_layers*2, batch_size, hidden_dim]\n",
        "\n",
        "        # Process encoder hidden state for decoder\n",
        "        encoder_hidden = encoder_hidden.view(self.encoder.num_layers, 2, batch_size, -1)\n",
        "        encoder_hidden = torch.cat([encoder_hidden[:, 0], encoder_hidden[:, 1]], dim=2)\n",
        "        # Now encoder_hidden: [num_layers, batch_size, hidden_dim*2]\n",
        "\n",
        "        # Decode\n",
        "        decoder_output, _ = self.decoder(tgt_embedded, encoder_hidden)\n",
        "        # decoder_output: [batch_size, tgt_len, hidden_dim*2]\n",
        "\n",
        "        # Apply attention over encoder outputs (optional, more advanced)\n",
        "        # This is a simplified attention mechanism\n",
        "        attn_weights = torch.bmm(decoder_output, encoder_outputs.transpose(1, 2))\n",
        "        attn_weights = F.softmax(attn_weights, dim=2)  # [batch_size, tgt_len, src_len]\n",
        "        context = torch.bmm(attn_weights, encoder_outputs)  # [batch_size, tgt_len, hidden_dim*2]\n",
        "\n",
        "        # Combine context with decoder output (simple concatenation+projection for this example)\n",
        "        combined = decoder_output + context\n",
        "\n",
        "        # Project to vocabulary size\n",
        "        output = self.fc_out(self.dropout(combined))  # [batch_size, tgt_len, vocab_size]\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "49aCy4mh0YgD"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = FitnessGRUBot(\n",
        "    vocab_size=len(word2index),\n",
        "    embed_dim=100,  # Match GloVe embedding dimension\n",
        "    hidden_dim=256,\n",
        "    num_layers=2,\n",
        "    dropout=0.01,\n",
        "    pretrained_embeddings=embedding_matrix\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o5JVnY9y0Yi6"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler=None, num_epochs=30, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for src, tgt in train_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(src, tgt[:, :-1])\n",
        "\n",
        "            # Flatten output and target for loss calculation\n",
        "            output_flat = output.contiguous().view(-1, output.size(-1))\n",
        "            target_flat = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output_flat, target_flat)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for src, tgt in val_loader:\n",
        "                src, tgt = src.to(device), tgt.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                output = model(src, tgt[:, :-1])\n",
        "\n",
        "                # Flatten output and target for loss calculation\n",
        "                output_flat = output.contiguous().view(-1, output.size(-1))\n",
        "                target_flat = tgt[:, 1:].contiguous().view(-1)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(output_flat, target_flat)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        # Calculate average losses\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        if scheduler:\n",
        "            scheduler.step(avg_val_loss)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKXZucxt0Ylz",
        "outputId": "93c73ada-4994-429b-e4b8-4f5023bac4ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.1)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='min', factor=0.5, verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "LSEtVHIj0Yoh"
      },
      "outputs": [],
      "source": [
        "def generate_response_beam(\n",
        "    model,\n",
        "    user_input,\n",
        "    max_length=20,\n",
        "    beam_width=3,\n",
        "    temperature=0.8,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "):\n",
        "    model.eval()\n",
        "    input_ids = torch.tensor([encode_text(user_input.lower(), word2index)], dtype=torch.long).to(device)\n",
        "\n",
        "    # Special token IDs\n",
        "    sos_token = word2index[\"<sos>\"]\n",
        "    eos_token = word2index[\"<eos>\"]\n",
        "    pad_token = word2index[\"<pad>\"]\n",
        "\n",
        "    # Initialize beam search\n",
        "    sequences = [(torch.tensor([[sos_token]], dtype=torch.long).to(device), 0.0)]\n",
        "    finished_sequences = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step in range(max_length):\n",
        "            all_candidates = []\n",
        "\n",
        "            for seq, score in sequences:\n",
        "                # Check if sequence has ended\n",
        "                if seq[0, -1].item() == eos_token or seq.size(1) >= max_length:\n",
        "                    finished_sequences.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Get model prediction\n",
        "                output = model(input_ids, seq)\n",
        "                logits = output[:, -1, :] / temperature\n",
        "                probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                # Get top k predictions\n",
        "                topk_probs, topk_indices = torch.topk(probs, beam_width)\n",
        "\n",
        "                # Create new candidate sequences\n",
        "                for i in range(beam_width):\n",
        "                    next_token_id = topk_indices[0, i].item()\n",
        "                    next_score = topk_probs[0, i].item()\n",
        "\n",
        "                    # Penalize repetition (simplified)\n",
        "                    if seq.size(1) >= 2 and next_token_id == seq[0, -1].item() == seq[0, -2].item():\n",
        "                        continue\n",
        "\n",
        "                    # Create new sequence\n",
        "                    new_seq = torch.cat([seq, torch.tensor([[next_token_id]], dtype=torch.long).to(device)], dim=1)\n",
        "\n",
        "                    # Calculate score with length normalization\n",
        "                    length_penalty = (5 + new_seq.size(1)) / 6\n",
        "                    adjusted_score = (score + next_score) / length_penalty\n",
        "\n",
        "                    all_candidates.append((new_seq, adjusted_score))\n",
        "\n",
        "            # If no candidates left, break\n",
        "            if not all_candidates:\n",
        "                break\n",
        "\n",
        "            # Keep top beam_width candidates\n",
        "            sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "        # Add any remaining sequences to finished\n",
        "        finished_sequences.extend(sequences)\n",
        "\n",
        "        # Get best sequence\n",
        "        if finished_sequences:\n",
        "            finished_sequences = sorted(finished_sequences, key=lambda x: x[1], reverse=True)\n",
        "            best_sequence = finished_sequences[0][0].squeeze().tolist()\n",
        "        else:\n",
        "            best_sequence = sequences[0][0].squeeze().tolist()\n",
        "\n",
        "        # Decode\n",
        "        decoded = [index2word.get(idx, \"<unk>\") for idx in best_sequence]\n",
        "\n",
        "        # Clean up\n",
        "        result = []\n",
        "        for token in decoded:\n",
        "            if token not in [\"<sos>\", \"<eos>\", \"<pad>\", \"<unk>\"]:\n",
        "                result.append(token)\n",
        "\n",
        "        return \" \".join(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "67adqKJz0YrW"
      },
      "outputs": [],
      "source": [
        "def post_process_response(response):\n",
        "    if not response or len(response.split()) < 3:\n",
        "        return \"I don't have enough information to answer that question properly.\"\n",
        "\n",
        "    # Basic cleaning\n",
        "    response = ' '.join(response.split())  # Remove multiple spaces\n",
        "\n",
        "    # Capitalize first letter\n",
        "    response = response[0].upper() + response[1:]\n",
        "\n",
        "    # Fix common issues\n",
        "    response = response.replace(\" i \", \" I \")\n",
        "\n",
        "    # Add period if missing final punctuation\n",
        "    if not response[-1] in ['.', '!', '?']:\n",
        "        response += '.'\n",
        "\n",
        "    # Remove repeated sentences if any\n",
        "    sentences = response.split('. ')\n",
        "    unique_sentences = []\n",
        "    for sentence in sentences:\n",
        "        if sentence not in unique_sentences:\n",
        "            unique_sentences.append(sentence)\n",
        "    response = '. '.join(unique_sentences)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wbaiEV5W0YuH"
      },
      "outputs": [],
      "source": [
        "def get_fallback_response(query):\n",
        "    query = query.lower()\n",
        "\n",
        "    # Expanded fallback dictionary\n",
        "    fallbacks = {\n",
        "        \"flexibility\": \"To improve flexibility, try daily stretching routines, yoga, or Pilates. Hold each stretch for 15-30 seconds and focus on major muscle groups. Consistency is key.\",\n",
        "\n",
        "        \"strength training\": \"A good beginner strength routine includes 2-3 sessions per week focusing on compound movements like squats, push-ups, rows, and lunges. Start with bodyweight exercises before adding weights.\",\n",
        "\n",
        "        \"water\": \"During workouts, aim to drink about 7-10 ounces of water every 10-20 minutes. For workouts under an hour, water is sufficient. For longer sessions, consider sports drinks to replace electrolytes.\",\n",
        "\n",
        "        \"supplement\": \"Supplements aren't necessary to get fit. Focus on a balanced diet with adequate protein, carbs, and healthy fats. If considering supplements, protein powder can be convenient, but whole foods should be your primary nutrition source.\",\n",
        "\n",
        "        \"cardio\": \"For cardiovascular fitness, aim for 150 minutes of moderate activity weekly. Options include walking, running, cycling, swimming, or any activity that elevates your heart rate.\",\n",
        "\n",
        "        \"protein\": \"Most active adults should aim for 0.6-0.8 grams of protein per pound of body weight daily. Good sources include lean meats, eggs, dairy, legumes, and plant-based options like tofu.\",\n",
        "\n",
        "        \"weight loss\": \"For healthy weight loss, focus on a moderate calorie deficit of 300-500 calories per day, combined with regular exercise. Aim for 1-2 pounds of weight loss per week.\",\n",
        "\n",
        "        \"recovery\": \"Proper recovery includes adequate sleep (7-9 hours), proper nutrition, hydration, and rest days between intense workouts. Active recovery like light walking or yoga can also help.\",\n",
        "\n",
        "        \"beginner\": \"As a beginner, start with 2-3 days of exercise per week combining basic strength movements and moderate cardio. Focus on proper form rather than intensity and gradually increase duration and difficulty.\"\n",
        "    }\n",
        "\n",
        "    # Check if any keywords match\n",
        "    for keyword, response in fallbacks.items():\n",
        "        if keyword in query:\n",
        "            return response\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hvzW-NsV0Yzd"
      },
      "outputs": [],
      "source": [
        "def chat_with_fitness_bot(model):\n",
        "    print(\"FitBot\")\n",
        "    print(\"Ask me any fitness questions! (Type 'exit' to quit)\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "            print(\"Chatbot: Thanks for chatting! Stay fit and healthy!\")\n",
        "            break\n",
        "\n",
        "        # First check if we should use a fallback response\n",
        "        fallback = get_fallback_response(user_input)\n",
        "        if fallback:\n",
        "            print(f\"Chatbot: {fallback}\")\n",
        "            continue\n",
        "\n",
        "        # Otherwise generate response with the model\n",
        "        try:\n",
        "            raw_response = generate_response_beam(model, user_input)\n",
        "            cleaned_response = post_process_response(raw_response)\n",
        "\n",
        "            # Quality check\n",
        "            if len(cleaned_response.split()) < 5 or cleaned_response.count(',') > 5:\n",
        "                print(\"Chatbot: That's a great fitness question. For personalized advice, consider consulting with a certified fitness professional.\")\n",
        "            else:\n",
        "                print(f\"Chatbot: {cleaned_response}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Chatbot: I'm having trouble processing that question. Could you rephrase it?\")\n",
        "            print(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgBhU3AU0Y2U",
        "outputId": "d61cce3c-eda0-40d4-b438-015d126c5bea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 1/30 - Train Loss: 6.6370, Val Loss: 6.0210\n",
            "Epoch 2/30 - Train Loss: 5.6309, Val Loss: 5.5345\n",
            "Epoch 3/30 - Train Loss: 5.0610, Val Loss: 5.3178\n",
            "Epoch 4/30 - Train Loss: 4.5683, Val Loss: 5.2324\n",
            "Epoch 5/30 - Train Loss: 4.0839, Val Loss: 5.2133\n",
            "Epoch 6/30 - Train Loss: 3.6267, Val Loss: 5.2354\n",
            "Epoch 7/30 - Train Loss: 3.2468, Val Loss: 5.2724\n",
            "Epoch 8/30 - Train Loss: 2.9287, Val Loss: 5.3273\n",
            "Epoch 9/30 - Train Loss: 2.6574, Val Loss: 5.3963\n",
            "Epoch 10/30 - Train Loss: 2.4233, Val Loss: 5.4723\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting training...\")\n",
        "trained_model = train_model(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    num_epochs=30,\n",
        ")\n",
        "\n",
        "# Save the final model\n",
        "torch.save(trained_model.state_dict(), \"final_fitbot_gru_model.pth\")\n",
        "print(\"Training complete! Model saved as 'final_fitbot_gru_model.pth'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdo59ugG0Y5H",
        "outputId": "edf6be17-b1d9-45e7-baae-e22e638900f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model successfully!\n",
            "FitBot\n",
            "Ask me any fitness questions! (Type 'exit' to quit)\n",
            "\n",
            "You: What are the benefits of cross-training?\n",
            "Chatbot: It reduces overuse injuries, improves overall fitness, and keeps workouts mentally engaging.\n",
            "No pretrained model found. Please train the model first.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    model.load_state_dict(torch.load(\"final_fitbot_gru_model.pth\"))\n",
        "    print(\"Loaded pretrained model successfully!\")\n",
        "    chat_with_fitness_bot(model)\n",
        "except:\n",
        "    print(\"No pretrained model found. Please train the model first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
